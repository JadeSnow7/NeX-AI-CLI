#include "nex/ai/ai_core.h"
#include <iostream>
#include <fstream>
#include <sstream>
#include <thread>
#include <chrono>
#include <regex>
#include <filesystem>
#include <cstdlib>
#include <unistd.h>
#include <sys/stat.h>
#include <sys/sysinfo.h>
#include <future>
#include <sys/wait.h>

namespace nex::ai {

// LlamaEngine Implementation (Placeholder for llama.cpp integration)
class LlamaEngine::Impl {
public:
    bool model_loaded = false;
    ModelConfig config;
    
    // TODO: é›†æˆå®é™…çš„ llama.cpp API
    // è¿™é‡Œå…ˆæä¾›å ä½ç¬¦å®ç°
    
    bool loadModel(const ModelConfig& cfg) {
        config = cfg;
        
        // æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if (!std::filesystem::exists(cfg.model_path)) {
            std::cerr << "Model file not found: " << cfg.model_path << std::endl;
            return false;
        }
        
        std::cout << "Loading model: " << cfg.model_path << std::endl;
        std::cout << "Model type: " << cfg.model_type << std::endl;
        std::cout << "Quantized: " << (cfg.quantized ? "Yes" : "No") << std::endl;
        
        // æ¨¡æ‹ŸåŠ è½½æ—¶é—´
        std::this_thread::sleep_for(std::chrono::milliseconds(500));
        
        model_loaded = true;
        return true;
    }
    
    std::string generate(const std::string& prompt, int max_tokens) {
        if (!model_loaded) {
            return "ERROR: Model not loaded";
        }
        
        // TODO: è°ƒç”¨å®é™…çš„ llama.cpp æ¨ç†
        // ç°åœ¨è¿”å›æ¨¡æ‹Ÿå“åº”
        
        if (prompt.find("ä¸‹è½½") != std::string::npos && prompt.find("gcc") != std::string::npos) {
            return "sudo apt update && sudo apt install gcc";
        }
        
        if (prompt.find("å®‰è£…") != std::string::npos && prompt.find("python") != std::string::npos) {
            return "sudo apt install python3 python3-pip";
        }
        
        if (prompt.find("æŸ¥æ‰¾") != std::string::npos && prompt.find(".cpp") != std::string::npos) {
            return "find . -name '*.cpp' -type f";
        }
        
        if (prompt.find("ç³»ç»ŸçŠ¶æ€") != std::string::npos || prompt.find("ç›‘æ§") != std::string::npos) {
            return "ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚CPUä½¿ç”¨ç‡è¾ƒä½ï¼Œå†…å­˜å……è¶³ï¼Œå»ºè®®å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶ã€‚";
        }
        
        return "æˆ‘ç†è§£æ‚¨çš„è¯·æ±‚ï¼Œä½†éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ç”Ÿæˆå‡†ç¡®çš„å‘½ä»¤ã€‚";
    }
};

LlamaEngine::LlamaEngine() : pImpl(std::make_unique<Impl>()) {}
LlamaEngine::~LlamaEngine() = default;

bool LlamaEngine::loadModel(const ModelConfig& config) {
    return pImpl->loadModel(config);
}

bool LlamaEngine::unloadModel() {
    pImpl->model_loaded = false;
    return true;
}

bool LlamaEngine::isModelLoaded() const {
    return pImpl->model_loaded;
}

std::string LlamaEngine::generate(const std::string& prompt, int max_tokens) {
    return pImpl->generate(prompt, max_tokens);
}

std::future<std::string> LlamaEngine::generateAsync(const std::string& prompt, int max_tokens) {
    return std::async(std::launch::async, [this, prompt, max_tokens]() {
        return this->generate(prompt, max_tokens);
    });
}

void LlamaEngine::generateStream(const std::string& prompt, 
                                std::function<void(const std::string&)> callback,
                                int max_tokens) {
    // æ¨¡æ‹Ÿæµå¼è¾“å‡º
    std::string response = generate(prompt, max_tokens);
    
    // åˆ†å—å‘é€
    const size_t chunk_size = 10;
    for (size_t i = 0; i < response.length(); i += chunk_size) {
        std::string chunk = response.substr(i, chunk_size);
        callback(chunk);
        std::this_thread::sleep_for(std::chrono::milliseconds(50));
    }
}

std::string LlamaEngine::getModelInfo() const {
    if (!pImpl->model_loaded) {
        return "No model loaded";
    }
    
    std::stringstream ss;
    ss << "Model: " << pImpl->config.model_type << std::endl;
    ss << "Path: " << pImpl->config.model_path << std::endl;
    ss << "Context Length: " << pImpl->config.context_length << std::endl;
    ss << "Quantized: " << (pImpl->config.quantized ? "Yes" : "No") << std::endl;
    return ss.str();
}

size_t LlamaEngine::getMemoryUsage() const {
    // TODO: è¿”å›å®é™…å†…å­˜ä½¿ç”¨é‡
    return pImpl->model_loaded ? 2048 * 1024 * 1024 : 0; // 2GB æ¨¡æ‹Ÿå€¼
}

// CommandParser Implementation
CommandParser::CommandParser(std::shared_ptr<LlamaEngine> engine) 
    : ai_engine_(engine) {
    
    // åˆå§‹åŒ–é¢„å®šä¹‰å‘½ä»¤æ˜ å°„
    CommandMapping gcc_install = {
        "ä¸‹è½½gcc", "sudo apt install gcc", "å®‰è£…GCCç¼–è¯‘å™¨",
        {"å®‰è£…gcc", "gccä¸‹è½½", "å®‰è£…ç¼–è¯‘å™¨"}, true, "package"
    };
    command_mappings_["gcc"] = gcc_install;
    
    CommandMapping python_install = {
        "å®‰è£…python", "sudo apt install python3 python3-pip", "å®‰è£…Pythonè§£é‡Šå™¨",
        {"ä¸‹è½½python", "pythonå®‰è£…", "å®‰è£…python3"}, true, "package"
    };
    command_mappings_["python"] = python_install;
    
    CommandMapping find_cpp = {
        "æŸ¥æ‰¾cppæ–‡ä»¶", "find . -name '*.cpp' -type f", "æŸ¥æ‰¾æ‰€æœ‰C++æºæ–‡ä»¶",
        {"æ‰¾cpp", "æœç´¢cpp", "åˆ—å‡ºcppæ–‡ä»¶"}, false, "file"
    };
    command_mappings_["find_cpp"] = find_cpp;
}

AIResponse CommandParser::parseCommand(const std::string& natural_input) {
    AIResponse response;
    response.success = false;
    
    if (!ai_engine_ || !ai_engine_->isModelLoaded()) {
        response.error_message = "AIæ¨¡å‹æœªåŠ è½½";
        return response;
    }
    
    // é¦–å…ˆå°è¯•åŒ¹é…é¢„å®šä¹‰å‘½ä»¤
    auto matches = findMatches(natural_input);
    if (!matches.empty()) {
        response.success = true;
        response.command = matches[0].shell_command;
        response.content = matches[0].description;
        response.confidence = 0.9f;
        return response;
    }
    
    // ä½¿ç”¨AIç”Ÿæˆå‘½ä»¤
    std::string prompt = buildPrompt(natural_input);
    std::string ai_response = ai_engine_->generate(prompt, 256);
    
    response.success = true;
    response.command = ai_response;
    response.content = "AIç”Ÿæˆçš„å‘½ä»¤";
    response.confidence = 0.7f;
    
    return response;
}

std::string CommandParser::buildPrompt(const std::string& input, const std::string& context) {
    std::stringstream prompt;
    prompt << "ä½ æ˜¯ä¸€ä¸ªLinuxå‘½ä»¤è¡Œä¸“å®¶ã€‚ç”¨æˆ·è¯´ï¼š\"" << input << "\"" << std::endl;
    prompt << "è¯·ç”Ÿæˆå¯¹åº”çš„shellå‘½ä»¤ï¼Œåªè¿”å›å‘½ä»¤æœ¬èº«ï¼Œä¸è¦è§£é‡Šã€‚" << std::endl;
    prompt << "å¦‚æœéœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œè¯·åŠ ä¸Šsudoã€‚" << std::endl;
    prompt << "å¸¸è§æ˜ å°„ï¼š" << std::endl;
    prompt << "- ä¸‹è½½/å®‰è£…gcc -> sudo apt install gcc" << std::endl;
    prompt << "- å®‰è£…python -> sudo apt install python3 python3-pip" << std::endl;
    prompt << "- æŸ¥æ‰¾æ–‡ä»¶ -> findå‘½ä»¤" << std::endl;
    prompt << "- æŸ¥çœ‹çŠ¶æ€ -> ps, top, dfç­‰" << std::endl;
    prompt << std::endl << "å‘½ä»¤ï¼š";
    
    return prompt.str();
}

std::vector<CommandMapping> CommandParser::findMatches(const std::string& input) {
    std::vector<CommandMapping> matches;
    
    for (const auto& [key, mapping] : command_mappings_) {
        // æ£€æŸ¥ä¸»è¦å…³é”®è¯
        if (input.find("gcc") != std::string::npos && key == "gcc") {
            matches.push_back(mapping);
        } else if (input.find("python") != std::string::npos && key == "python") {
            matches.push_back(mapping);
        } else if (input.find("cpp") != std::string::npos && key == "find_cpp") {
            matches.push_back(mapping);
        }
        
        // æ£€æŸ¥åˆ«å
        for (const auto& alias : mapping.aliases) {
            if (input.find(alias) != std::string::npos) {
                matches.push_back(mapping);
                break;
            }
        }
    }
    
    return matches;
}

// SystemMonitorAI Implementation
SystemMonitorAI::SystemMonitorAI(std::shared_ptr<LlamaEngine> engine) 
    : ai_engine_(engine) {}

SystemStatus SystemMonitorAI::collectSystemStatus() {
    SystemStatus status;
    
    status.cpu_usage = getCPUUsage();
    status.memory_usage = getMemoryUsage();
    status.disk_usage = getDiskUsage();
    status.running_processes = getRunningProcesses();
    status.system_info = getSystemInfo();
    status.timestamp = std::to_string(std::time(nullptr));
    
    // ä¿å­˜åˆ°å†å²è®°å½•
    status_history_.push_back(status);
    if (status_history_.size() > 100) {
        status_history_.erase(status_history_.begin());
    }
    
    return status;
}

float SystemMonitorAI::getCPUUsage() {
    // ç®€å•çš„CPUä½¿ç”¨ç‡è·å–
    std::ifstream stat_file("/proc/stat");
    if (!stat_file.is_open()) return -1.0f;
    
    std::string line;
    std::getline(stat_file, line);
    
    // è§£æ /proc/stat ç¬¬ä¸€è¡Œ
    std::istringstream iss(line);
    std::string cpu;
    long user, nice, system, idle;
    iss >> cpu >> user >> nice >> system >> idle;
    
    static long prev_idle = 0, prev_total = 0;
    long total = user + nice + system + idle;
    long diff_idle = idle - prev_idle;
    long diff_total = total - prev_total;
    
    float usage = 100.0f * (1.0f - (float)diff_idle / diff_total);
    
    prev_idle = idle;
    prev_total = total;
    
    return usage;
}

float SystemMonitorAI::getMemoryUsage() {
    struct sysinfo info;
    if (sysinfo(&info) != 0) return -1.0f;
    
    long total_mem = info.totalram * info.mem_unit;
    long free_mem = info.freeram * info.mem_unit;
    long used_mem = total_mem - free_mem;
    
    return 100.0f * used_mem / total_mem;
}

float SystemMonitorAI::getDiskUsage() {
    std::filesystem::space_info space = std::filesystem::space(".");
    if (space.capacity == static_cast<std::uintmax_t>(-1)) return -1.0f;
    
    return 100.0f * (space.capacity - space.available) / space.capacity;
}

std::vector<std::string> SystemMonitorAI::getRunningProcesses() {
    std::vector<std::string> processes;
    
    // è·å–å½“å‰ç”¨æˆ·çš„è¿›ç¨‹
    FILE* pipe = popen("ps -u $(whoami) -o pid,comm,pcpu --no-headers | head -10", "r");
    if (!pipe) return processes;
    
    char buffer[256];
    while (fgets(buffer, sizeof(buffer), pipe)) {
        processes.push_back(std::string(buffer));
    }
    pclose(pipe);
    
    return processes;
}

std::map<std::string, std::string> SystemMonitorAI::getSystemInfo() {
    std::map<std::string, std::string> info;
    
    // è·å–ç³»ç»Ÿä¿¡æ¯
    char hostname[256];
    gethostname(hostname, sizeof(hostname));
    info["hostname"] = hostname;
    
    info["user"] = getenv("USER") ? getenv("USER") : "unknown";
    
    // è·å–ç³»ç»Ÿç‰ˆæœ¬
    std::ifstream version_file("/proc/version");
    if (version_file.is_open()) {
        std::string version;
        std::getline(version_file, version);
        info["kernel"] = version.substr(0, version.find(' ', 14)); // ç®€åŒ–ç‰ˆæœ¬ä¿¡æ¯
    }
    
    return info;
}

AIResponse SystemMonitorAI::analyzeSystemStatus(const SystemStatus& status) {
    AIResponse response;
    
    if (!ai_engine_ || !ai_engine_->isModelLoaded()) {
        response.success = false;
        response.error_message = "AIæ¨¡å‹æœªåŠ è½½";
        return response;
    }
    
    std::stringstream prompt;
    prompt << "åˆ†æä»¥ä¸‹ç³»ç»ŸçŠ¶æ€å¹¶æä¾›ä¼˜åŒ–å»ºè®®ï¼š" << std::endl;
    prompt << "CPUä½¿ç”¨ç‡: " << status.cpu_usage << "%" << std::endl;
    prompt << "å†…å­˜ä½¿ç”¨ç‡: " << status.memory_usage << "%" << std::endl;
    prompt << "ç£ç›˜ä½¿ç”¨ç‡: " << status.disk_usage << "%" << std::endl;
    prompt << "è¿è¡Œè¿›ç¨‹æ•°: " << status.running_processes.size() << std::endl;
    prompt << "è¯·æä¾›ç®€æ´çš„åˆ†æå’Œå»ºè®®ã€‚";
    
    std::string ai_analysis = ai_engine_->generate(prompt.str(), 512);
    
    response.success = true;
    response.content = ai_analysis;
    return response;
}

// AIManager Implementation
AIManager& AIManager::getInstance() {
    static AIManager instance;
    return instance;
}

bool AIManager::initialize(const std::string& config_file) {
    if (initialized_) return true;
    
    // åˆ›å»ºAIå¼•æ“
    engine_ = std::make_shared<LlamaEngine>();
    
    // åŠ è½½é…ç½®
    ModelConfig config;
    config.model_path = "/path/to/qwen3-coder-model.gguf";  // TODO: ä»é…ç½®æ–‡ä»¶è¯»å–
    config.model_type = "qwen3-coder";
    config.quantized = true;
    config.n_threads = std::thread::hardware_concurrency();
    
    // è¿™é‡Œå…ˆä¸å®é™…åŠ è½½æ¨¡å‹ï¼Œç­‰ llama.cpp é›†æˆå®Œæˆ
    std::cout << "AIç³»ç»Ÿåˆå§‹åŒ–ä¸­..." << std::endl;
    std::cout << "æ¨¡å‹ç±»å‹: " << config.model_type << std::endl;
    std::cout << "é‡åŒ–æ¨¡å¼: " << (config.quantized ? "å¯ç”¨" : "ç¦ç”¨") << std::endl;
    
    // åˆ›å»ºå„ä¸ªç»„ä»¶
    command_parser_ = std::make_shared<CommandParser>(engine_);
    system_monitor_ = std::make_shared<SystemMonitorAI>(engine_);
    conversation_manager_ = std::make_shared<ConversationManager>(engine_);
    command_executor_ = std::make_shared<AICommandExecutor>(command_parser_);
    
    initialized_ = true;
    std::cout << "AIç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼" << std::endl;
    
    return true;
}

AIResponse AIManager::processUserInput(const std::string& input) {
    if (!initialized_) {
        AIResponse response;
        response.success = false;
        response.error_message = "AIç³»ç»Ÿæœªåˆå§‹åŒ–";
        return response;
    }
    
    // è§£æç”¨æˆ·è¾“å…¥
    return command_parser_->parseCommand(input);
}

AIResponse AIManager::analyzeSystem() {
    if (!initialized_) {
        AIResponse response;
        response.success = false;
        response.error_message = "AIç³»ç»Ÿæœªåˆå§‹åŒ–";
        return response;
    }
    
    auto status = system_monitor_->collectSystemStatus();
    return system_monitor_->analyzeSystemStatus(status);
}

std::string AIManager::getStatus() const {
    if (!initialized_) return "æœªåˆå§‹åŒ–";
    
    std::stringstream ss;
    ss << "AIç³»ç»ŸçŠ¶æ€: è¿è¡Œä¸­" << std::endl;
    ss << "æ¨¡å‹åŠ è½½: " << (engine_->isModelLoaded() ? "å·²åŠ è½½" : "æœªåŠ è½½") << std::endl;
    ss << "å†…å­˜ä½¿ç”¨: " << (engine_->getMemoryUsage() / 1024 / 1024) << " MB" << std::endl;
    
    return ss.str();
}

// ConversationManager Implementation
ConversationManager::ConversationManager(std::shared_ptr<LlamaEngine> engine) 
    : ai_engine_(engine) {}

void ConversationManager::startConversation() {
    conversation_history_.clear();
    current_context_ = "ç”¨æˆ·å¼€å§‹äº†æ–°çš„å¯¹è¯ä¼šè¯";
}

AIResponse ConversationManager::processInput(const std::string& input) {
    AIResponse response;
    
    if (!ai_engine_ || !ai_engine_->isModelLoaded()) {
        response.success = false;
        response.error_message = "AIæ¨¡å‹æœªåŠ è½½";
        return response;
    }
    
    // æ„å»ºå¯¹è¯æç¤ºè¯
    std::string prompt = buildConversationPrompt(input);
    
    // ç”Ÿæˆå“åº”
    std::string ai_response = ai_engine_->generate(prompt, 512);
    
    // ä¿å­˜å¯¹è¯å†å²
    conversation_history_.emplace_back(input, ai_response);
    if (conversation_history_.size() > 10) {
        conversation_history_.erase(conversation_history_.begin());
    }
    
    response.success = true;
    response.content = ai_response;
    return response;
}

std::string ConversationManager::buildConversationPrompt(const std::string& input) {
    std::stringstream prompt;
    prompt << "ä½ æ˜¯NeX AIåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·å¤„ç†Linuxå‘½ä»¤è¡Œä»»åŠ¡ã€‚" << std::endl;
    
    // æ·»åŠ å¯¹è¯å†å²
    if (!conversation_history_.empty()) {
        prompt << "å¯¹è¯å†å²ï¼š" << std::endl;
        for (const auto& [user_input, ai_response] : conversation_history_) {
            prompt << "ç”¨æˆ·: " << user_input << std::endl;
            prompt << "åŠ©æ‰‹: " << ai_response << std::endl;
        }
    }
    
    prompt << "å½“å‰ç”¨æˆ·è¯´: " << input << std::endl;
    prompt << "è¯·å›åº”ç”¨æˆ·å¹¶æä¾›æœ‰ç”¨çš„å»ºè®®æˆ–å‘½ä»¤ï¼š" << std::endl;
    
    return prompt.str();
}

// AICommandExecutor Implementation
AICommandExecutor::AICommandExecutor(std::shared_ptr<CommandParser> parser) 
    : parser_(parser) {
    initializeDangerousCommands();
}

void AICommandExecutor::initializeDangerousCommands() {
    dangerous_commands_ = {
        "rm -rf /", "rm -rf /*", ":(){ :|:& };:", 
        "mkfs", "dd if=", "shred", "wipefs",
        "chmod -R 777 /", "chown -R"
    };
}

AIResponse AICommandExecutor::executeNaturalCommand(const std::string& input, bool confirm) {
    AIResponse parse_result = parser_->parseCommand(input);
    
    if (!parse_result.success) {
        return parse_result;
    }
    
    std::string command = parse_result.command;
    
    // å®‰å…¨æ£€æŸ¥
    if (safe_mode_ && !isSafeCommand(command)) {
        parse_result.success = false;
        parse_result.error_message = "æ£€æµ‹åˆ°å±é™©å‘½ä»¤ï¼Œæ‹’ç»æ‰§è¡Œ: " + command;
        return parse_result;
    }
    
    // ç”¨æˆ·ç¡®è®¤
    if (confirm && !requestUserConfirmation(command, parse_result.content)) {
        parse_result.success = false;
        parse_result.error_message = "ç”¨æˆ·å–æ¶ˆæ‰§è¡Œ";
        return parse_result;
    }
    
    // æ‰§è¡Œå‘½ä»¤
    auto [exit_code, output] = executeShellCommand(command);
    
    AIResponse response;
    response.success = (exit_code == 0);
    response.command = command;
    response.content = output;
    response.error_message = (exit_code != 0) ? "å‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼Œé€€å‡ºç : " + std::to_string(exit_code) : "";
    
    return response;
}

bool AICommandExecutor::isSafeCommand(const std::string& command) {
    for (const auto& dangerous : dangerous_commands_) {
        if (command.find(dangerous) != std::string::npos) {
            return false;
        }
    }
    return true;
}

bool AICommandExecutor::requestUserConfirmation(const std::string& command, const std::string& description) {
    std::cout << std::endl;
    std::cout << "ğŸ¤– AIå»ºè®®æ‰§è¡Œä»¥ä¸‹å‘½ä»¤:" << std::endl;
    std::cout << "ğŸ“‹ æè¿°: " << description << std::endl;
    std::cout << "ğŸ’» å‘½ä»¤: " << command << std::endl;
    std::cout << std::endl;
    std::cout << "æ˜¯å¦æ‰§è¡Œ? [Y/n]: ";
    
    std::string response;
    std::getline(std::cin, response);
    
    return response.empty() || response == "y" || response == "Y" || response == "yes";
}

std::pair<int, std::string> AICommandExecutor::executeShellCommand(const std::string& command) {
    std::string full_command = command + " 2>&1";
    FILE* pipe = popen(full_command.c_str(), "r");
    
    if (!pipe) {
        return {-1, "æ— æ³•æ‰§è¡Œå‘½ä»¤"};
    }
    
    std::string output;
    char buffer[256];
    while (fgets(buffer, sizeof(buffer), pipe)) {
        output += buffer;
    }
    
    int exit_code = pclose(pipe);
    return {WEXITSTATUS(exit_code), output};
}

} // namespace nex::ai
