# NeX: AIæ¨¡å‹é›†æˆå®ç°æ–¹æ¡ˆ

## ğŸ¯ AIé›†æˆç›®æ ‡

åŸºäºæˆ‘ä»¬ç°æœ‰çš„ShellåŸºç¡€è®¾æ–½ï¼Œå®ç°æ™ºèƒ½å‘½ä»¤è¡ŒåŠ©æ‰‹ï¼Œæä¾›ï¼š
1. **è‡ªç„¶è¯­è¨€å‘½ä»¤ç”Ÿæˆ** - å°†ç”¨æˆ·æè¿°è½¬æ¢ä¸ºShellå‘½ä»¤
2. **æ™ºèƒ½é”™è¯¯è¯Šæ–­** - åˆ†æå‘½ä»¤å¤±è´¥åŸå› å¹¶æä¾›è§£å†³æ–¹æ¡ˆ
3. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥å»ºè®®** - åŸºäºå½“å‰ç¯å¢ƒçŠ¶æ€æä¾›ç›¸å…³å»ºè®®
4. **ç³»ç»Ÿåˆ†æåŠ©æ‰‹** - åŸºäºç›‘æ§æ•°æ®è¿›è¡Œæ™ºèƒ½åˆ†æ

## ğŸ”§ æŠ€æœ¯é€‰å‹å»ºè®®

### æ¨èæ–¹æ¡ˆï¼šæœ¬åœ°å°æ¨¡å‹ + llama.cpp

**ç†ç”±**:
- âœ… **éšç§å®‰å…¨**: æ•°æ®å®Œå…¨æœ¬åœ°å¤„ç†
- âœ… **å“åº”é€Ÿåº¦**: æ— ç½‘ç»œå»¶è¿Ÿ
- âœ… **æˆæœ¬æ•ˆç›Š**: æ— APIè°ƒç”¨è´¹ç”¨
- âœ… **ç¦»çº¿å¯ç”¨**: ä¸ä¾èµ–ç½‘ç»œè¿æ¥
- âœ… **å¯æ§æ€§å¼º**: æ¨¡å‹è¡Œä¸ºå®Œå…¨å¯æ§

### æ¨èæ¨¡å‹é€‰æ‹©

#### ä¸»æ¨èï¼šCodeLlama-7B-Instruct-GGUF
```bash
# æ¨¡å‹æ–‡ä»¶å¤§å°: ~4.5GB (Q4_K_Mé‡åŒ–)
# å†…å­˜éœ€æ±‚: ~6-8GB
# æ¨ç†é€Ÿåº¦: ~15-25 tokens/sec (CPU)
# ä¸“é•¿: ä»£ç ç”Ÿæˆã€Shellè„šæœ¬ã€ç³»ç»Ÿç®¡ç†
```

#### å¤‡é€‰æ–¹æ¡ˆï¼šPhi-3-Mini-GGUF
```bash
# æ¨¡å‹æ–‡ä»¶å¤§å°: ~2.3GB (Q4_K_Mé‡åŒ–)
# å†…å­˜éœ€æ±‚: ~4-6GB  
# æ¨ç†é€Ÿåº¦: ~25-40 tokens/sec (CPU)
# ä¸“é•¿: è½»é‡çº§ä»£ç åŠ©æ‰‹ã€å¿«é€Ÿå“åº”
```

## ğŸ—ï¸ å®ç°æ¶æ„

### 1. AIæ¨¡å—ç»“æ„

```cpp
namespace nex::ai {
    // æ ¸å¿ƒAIå¼•æ“
    class AIEngine {
    public:
        bool initialize(const AIConfig& config);
        std::string processQuery(const std::string& query, const AIContext& context);
        std::vector<std::string> generateCommands(const std::string& description);
        std::string diagnoseError(const std::string& failed_command, const std::string& error_output);
        void shutdown();
    };
    
    // æ¨¡å‹ç®¡ç†å™¨
    class ModelManager {
    public:
        bool loadModel(const std::string& model_path, ModelType type);
        bool isModelLoaded() const;
        std::string inference(const std::string& prompt, const InferenceParams& params);
        void unloadModel();
    };
    
    // ä¸Šä¸‹æ–‡æ„å»ºå™¨
    class ContextBuilder {
    public:
        AIContext buildContext();
        void updateContext(const SystemEvent& event);
    private:
        std::string getCurrentDirectory();
        GitStatus getGitStatus();
        SystemStatus getSystemStatus();
        std::vector<std::string> getRecentCommands();
    };
}
```

### 2. llama.cppé›†æˆå±‚

```cpp
#include "llama.h"

class LlamaCppEngine {
public:
    struct Config {
        std::string model_path;
        int n_ctx = 4096;        // ä¸Šä¸‹æ–‡é•¿åº¦
        int n_threads = -1;      // çº¿ç¨‹æ•° (-1ä¸ºè‡ªåŠ¨)
        float temperature = 0.1; // ç”Ÿæˆæ¸©åº¦
        int max_tokens = 512;    // æœ€å¤§ç”Ÿæˆé•¿åº¦
    };
    
    bool initialize(const Config& config);
    std::string generate(const std::string& prompt);
    void cleanup();
    
private:
    llama_model* model_ = nullptr;
    llama_context* ctx_ = nullptr;
    llama_sampling_context* sampling_ctx_ = nullptr;
    
    std::vector<llama_token> tokenize(const std::string& text);
    std::string detokenize(const std::vector<llama_token>& tokens);
};
```

### 3. AIåŠŸèƒ½å®ç°

#### è‡ªç„¶è¯­è¨€å‘½ä»¤ç”Ÿæˆ

```cpp
class CommandGenerator {
public:
    std::vector<CommandSuggestion> generateCommands(const std::string& description) {
        std::string prompt = buildCommandPrompt(description);
        std::string response = ai_engine_.processQuery(prompt, context_builder_.buildContext());
        return parseCommands(response);
    }
    
private:
    std::string buildCommandPrompt(const std::string& description) {
        std::ostringstream prompt;
        prompt << "You are a Linux shell expert. Generate shell commands for the following task:\n\n";
        prompt << "Task: " << description << "\n\n";
        prompt << "Current directory: " << context_builder_.getCurrentDirectory() << "\n";
        prompt << "Available tools: " << getAvailableTools() << "\n\n";
        prompt << "Provide 1-3 shell commands that accomplish this task. ";
        prompt << "Format each command on a new line starting with '$ '.\n\n";
        prompt << "Commands:\n";
        return prompt.str();
    }
};
```

#### é”™è¯¯è¯Šæ–­ç³»ç»Ÿ

```cpp
class ErrorDiagnostic {
public:
    DiagnosticResult diagnoseCommand(const std::string& command, 
                                   const std::string& error_output,
                                   int exit_code) {
        std::string prompt = buildDiagnosticPrompt(command, error_output, exit_code);
        std::string analysis = ai_engine_.processQuery(prompt, context_builder_.buildContext());
        return parseDiagnostic(analysis);
    }
    
private:
    std::string buildDiagnosticPrompt(const std::string& command,
                                    const std::string& error_output,
                                    int exit_code) {
        std::ostringstream prompt;
        prompt << "Analyze this failed shell command and provide diagnostic information:\n\n";
        prompt << "Command: " << command << "\n";
        prompt << "Exit code: " << exit_code << "\n";
        prompt << "Error output:\n" << error_output << "\n\n";
        prompt << "System context:\n";
        prompt << "- Current directory: " << context_builder_.getCurrentDirectory() << "\n";
        prompt << "- User: " << context_builder_.getCurrentUser() << "\n";
        prompt << "- Available disk space: " << context_builder_.getDiskSpace() << "\n\n";
        prompt << "Provide:\n";
        prompt << "1. Root cause analysis\n";
        prompt << "2. Specific solution steps\n";
        prompt << "3. Alternative commands if applicable\n\n";
        prompt << "Analysis:\n";
        return prompt.str();
    }
};
```

### 4. é›†æˆåˆ°ç°æœ‰Shell

#### æ‰©å±•Shellå‘½ä»¤

```cpp
// åœ¨CommandExecutorä¸­æ·»åŠ AIå‘½ä»¤
void CommandExecutor::setupAICommands() {
    registerBuiltin("ai", [this](const ParsedCommand& cmd) -> CommandResult {
        if (cmd.args.empty()) {
            return CommandResult(1, "", "Usage: ai <query|explain|fix> <text>");
        }
        
        std::string subcommand = cmd.args[0];
        
        if (subcommand == "query" || subcommand == "q") {
            return handleAIQuery(cmd);
        } else if (subcommand == "explain" || subcommand == "e") {
            return handleExplainCommand(cmd);
        } else if (subcommand == "fix" || subcommand == "f") {
            return handleFixCommand(cmd);
        } else {
            // é»˜è®¤ä½œä¸ºæŸ¥è¯¢å¤„ç†
            return handleAIQuery(cmd);
        }
    });
}
```

#### æ™ºèƒ½æç¤ºé›†æˆ

```cpp
// æ‰©å±•AutoCompleter
class AIAutoCompleter : public AutoCompleter {
public:
    std::vector<std::string> getAISuggestions(const std::string& partial_command) {
        if (partial_command.empty()) return {};
        
        std::string prompt = "Complete this shell command: " + partial_command;
        auto suggestions = ai_engine_.generateCommands(prompt);
        
        std::vector<std::string> completions;
        for (const auto& suggestion : suggestions) {
            if (suggestion.find(partial_command) == 0) {
                completions.push_back(suggestion.substr(partial_command.length()));
            }
        }
        return completions;
    }
};
```

## ğŸ’» ç”¨æˆ·ç•Œé¢è®¾è®¡

### 1. AIå‘½ä»¤æ¥å£

```bash
# è‡ªç„¶è¯­è¨€æŸ¥è¯¢
nex ai "æ‰¾åˆ°æ‰€æœ‰Pythonæ–‡ä»¶å¹¶ç»Ÿè®¡è¡Œæ•°"
# â†’ find . -name "*.py" -exec wc -l {} + | tail -1

# å‘½ä»¤è§£é‡Š
nex ai explain "rsync -avz --delete src/ backup/"
# â†’ AIè§£é‡Š: è¿™ä¸ªå‘½ä»¤ä½¿ç”¨rsyncåŒæ­¥æ–‡ä»¶...

# é”™è¯¯ä¿®å¤
ls /nonexistent 2>&1 | nex ai fix
# â†’ AIå»ºè®®: ç›®å½•ä¸å­˜åœ¨ï¼Œå»ºè®®ä½¿ç”¨mkdiråˆ›å»ºæˆ–æ£€æŸ¥è·¯å¾„

# äº¤äº’å¼å¯¹è¯
nex ai chat
# è¿›å…¥AIåŠ©æ‰‹å¯¹è¯æ¨¡å¼
```

### 2. æ™ºèƒ½æç¤ºç¬¦

```bash
# åœ¨å‘½ä»¤å¤±è´¥åè‡ªåŠ¨æ˜¾ç¤ºAIå»ºè®®
user@host:~$ rm -rf /important/file
rm: cannot remove '/important/file': Permission denied

ğŸ¤– AIå»ºè®®: éœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œå°è¯•ï¼šsudo rm -rf /important/file
âš ï¸  è­¦å‘Š: æ­¤æ“ä½œä¸å¯é€†ï¼Œå»ºè®®å…ˆå¤‡ä»½

user@host:~$ 
```

### 3. ä¸Šä¸‹æ–‡æ„ŸçŸ¥å»ºè®®

```bash
# åŸºäºå½“å‰çŠ¶æ€çš„æ™ºèƒ½å»ºè®®
user@host:~/project (main)$ git 
ğŸ¤– æ£€æµ‹åˆ°Gitä»“åº“ï¼Œå¸¸ç”¨æ“ä½œ:
   git status    - æŸ¥çœ‹å½“å‰çŠ¶æ€
   git add .     - æ·»åŠ æ‰€æœ‰æ›´æ”¹
   git commit    - æäº¤æ›´æ”¹
   git push      - æ¨é€åˆ°è¿œç¨‹

user@host:~/project (main)$ 
```

## ğŸ”§ é…ç½®å’Œè‡ªå®šä¹‰

### AIé…ç½®æ‰©å±•

```ini
[ai.engine]
enabled = true
model_type = llama_cpp
model_path = ~/.nex/models/codellama-7b-instruct.q4_k_m.gguf
max_memory = 8192  # MB
threads = 4

[ai.features]
command_suggestions = true
error_diagnosis = true
auto_explain = false
smart_completion = true

[ai.behavior]
temperature = 0.1
max_tokens = 512
context_length = 4096
response_timeout = 30  # seconds

[ai.safety]
dangerous_commands_warning = true
auto_execute = false  # éœ€è¦ç”¨æˆ·ç¡®è®¤
sandbox_mode = true
```

### æ¨¡å‹ç®¡ç†

```bash
# æ¨¡å‹ç®¡ç†å‘½ä»¤
nex ai model list                    # åˆ—å‡ºå¯ç”¨æ¨¡å‹
nex ai model download codellama-7b   # ä¸‹è½½æ¨¡å‹
nex ai model switch phi3-mini        # åˆ‡æ¢æ¨¡å‹
nex ai model info                    # å½“å‰æ¨¡å‹ä¿¡æ¯
```

## ğŸ“Š æ€§èƒ½å’Œèµ„æºä¼˜åŒ–

### 1. æ‡’åŠ è½½ç­–ç•¥

```cpp
class LazyModelLoader {
public:
    void enableLazyLoading(bool enable) { lazy_loading_ = enable; }
    
    std::string processQuery(const std::string& query) {
        if (!model_loaded_ && lazy_loading_) {
            loadModelIfNeeded();
        }
        return ai_engine_.processQuery(query);
    }
    
private:
    void loadModelIfNeeded() {
        if (!model_loaded_) {
            std::cout << "ğŸ¤– æ­£åœ¨åŠ è½½AIæ¨¡å‹..." << std::endl;
            model_manager_.loadModel(config_.model_path);
            model_loaded_ = true;
        }
    }
    
    void scheduleUnload() {
        // 5åˆ†é’Ÿæ— æ´»åŠ¨åå¸è½½æ¨¡å‹
        unload_timer_.schedule(std::chrono::minutes(5), [this]() {
            if (shouldUnload()) {
                model_manager_.unloadModel();
                model_loaded_ = false;
            }
        });
    }
};
```

### 2. å“åº”ç¼“å­˜

```cpp
class ResponseCache {
public:
    std::optional<std::string> getCachedResponse(const std::string& query) {
        auto hash = std::hash<std::string>{}(query);
        auto it = cache_.find(hash);
        return (it != cache_.end()) ? std::make_optional(it->second) : std::nullopt;
    }
    
    void cacheResponse(const std::string& query, const std::string& response) {
        auto hash = std::hash<std::string>{}(query);
        cache_[hash] = response;
        
        // LRUæ¸…ç†
        if (cache_.size() > max_cache_size_) {
            cleanupCache();
        }
    }
};
```

## ğŸ›¡ï¸ å®‰å…¨æ€§è€ƒè™‘

### 1. å‘½ä»¤å®‰å…¨æ£€æŸ¥

```cpp
class CommandSafetyChecker {
public:
    enum class SafetyLevel {
        SAFE,       // æ— é£é™©å‘½ä»¤
        CAUTION,    // éœ€è¦æ³¨æ„çš„å‘½ä»¤
        DANGEROUS   // å±é™©å‘½ä»¤
    };
    
    SafetyLevel checkCommand(const std::string& command) {
        // å±é™©å‘½ä»¤æ¨¡å¼åŒ¹é…
        static const std::vector<std::regex> dangerous_patterns = {
            std::regex(R"(rm\s+.*-rf\s+/)"),           // rm -rf /
            std::regex(R"(dd\s+.*of=/)"),              // ddå†™å…¥ç³»ç»Ÿåˆ†åŒº
            std::regex(R"(mkfs\.)"),                   // æ ¼å¼åŒ–å‘½ä»¤
            std::regex(R"(:(){ :|:& };:)"),           // forkç‚¸å¼¹
        };
        
        for (const auto& pattern : dangerous_patterns) {
            if (std::regex_search(command, pattern)) {
                return SafetyLevel::DANGEROUS;
            }
        }
        
        return SafetyLevel::SAFE;
    }
};
```

### 2. æ²™ç›’é¢„è§ˆ

```cpp
class CommandSandbox {
public:
    std::string previewCommand(const std::string& command) {
        // åœ¨åªè¯»ç¯å¢ƒä¸­é¢„è§ˆå‘½ä»¤æ•ˆæœ
        std::string preview_env = createPreviewEnvironment();
        return executeInSandbox(command, preview_env);
    }
    
private:
    std::string createPreviewEnvironment() {
        // åˆ›å»ºä¸´æ—¶çš„åªè¯»æ–‡ä»¶ç³»ç»Ÿè§†å›¾
        // ä½¿ç”¨namespaceæˆ–chrootæŠ€æœ¯
    }
};
```

## ğŸš€ å®æ–½è®¡åˆ’

### Phase 1: åŸºç¡€AIé›†æˆ (2-3å‘¨)
1. âœ… **llama.cppé›†æˆ** - é›†æˆæ¨ç†å¼•æ“
2. âœ… **åŸºç¡€AIå‘½ä»¤** - å®ç°`nex ai`å‘½ä»¤æ¥å£
3. âœ… **é…ç½®ç³»ç»Ÿæ‰©å±•** - æ·»åŠ AIç›¸å…³é…ç½®
4. âœ… **ç®€å•æŸ¥è¯¢åŠŸèƒ½** - å®ç°åŸºç¡€è‡ªç„¶è¯­è¨€æŸ¥è¯¢

### Phase 2: æ ¸å¿ƒåŠŸèƒ½ (3-4å‘¨)
1. âœ… **å‘½ä»¤ç”Ÿæˆå™¨** - è‡ªç„¶è¯­è¨€è½¬Shellå‘½ä»¤
2. âœ… **é”™è¯¯è¯Šæ–­** - æ™ºèƒ½é”™è¯¯åˆ†æå’Œä¿®å¤å»ºè®®
3. âœ… **æ™ºèƒ½è¡¥å…¨** - AIå¢å¼ºçš„å‘½ä»¤è¡¥å…¨
4. âœ… **å®‰å…¨æ£€æŸ¥** - å±é™©å‘½ä»¤è¯†åˆ«å’Œè­¦å‘Š

### Phase 3: é«˜çº§ç‰¹æ€§ (4-5å‘¨)
1. âœ… **ä¸Šä¸‹æ–‡æ„ŸçŸ¥** - åŸºäºç¯å¢ƒçŠ¶æ€çš„æ™ºèƒ½å»ºè®®
2. âœ… **å­¦ä¹ ç³»ç»Ÿ** - ä»ç”¨æˆ·è¡Œä¸ºä¸­å­¦ä¹ 
3. âœ… **æ€§èƒ½ä¼˜åŒ–** - ç¼“å­˜ã€æ‡’åŠ è½½ç­‰ä¼˜åŒ–
4. âœ… **æ’ä»¶ç³»ç»Ÿ** - æ”¯æŒAIåŠŸèƒ½æ‰©å±•

---

## ğŸ¤” è®¨è®ºè¦ç‚¹

ç°åœ¨æˆ‘ä»¬æœ‰äº†å®Œæ•´çš„AIé›†æˆæ–¹æ¡ˆï¼Œæˆ‘æƒ³å’Œä½ è®¨è®ºå‡ ä¸ªå…³é”®é—®é¢˜ï¼š

1. **æ¨¡å‹é€‰æ‹©**: ä½ å€¾å‘äºå“ªä¸ªæ¨¡å‹ï¼ŸCodeLlamaä¸“ä¸šæ€§å¼ºä½†ä½“ç§¯å¤§ï¼ŒPhi-3è½»é‡ä½†é€šç”¨æ€§æ›´å¼º

2. **åŠŸèƒ½ä¼˜å…ˆçº§**: ä½ è®¤ä¸ºåº”è¯¥ä¼˜å…ˆå®ç°å“ªä¸ªAIåŠŸèƒ½ï¼Ÿ
   - è‡ªç„¶è¯­è¨€å‘½ä»¤ç”Ÿæˆ
   - æ™ºèƒ½é”™è¯¯è¯Šæ–­  
   - ä¸Šä¸‹æ–‡æ„ŸçŸ¥å»ºè®®
   - ç³»ç»Ÿåˆ†æåŠ©æ‰‹

3. **ç”¨æˆ·ä½“éªŒ**: ä½ å¸Œæœ›AIåŠ©æ‰‹æ˜¯ï¼š
   - ä¸»åŠ¨æç¤ºå‹ï¼ˆè‡ªåŠ¨æ˜¾ç¤ºå»ºè®®ï¼‰
   - è¢«åŠ¨å“åº”å‹ï¼ˆç”¨æˆ·ä¸»åŠ¨è¯¢é—®ï¼‰
   - æ··åˆæ¨¡å¼

4. **èµ„æºçº¦æŸ**: ä½ çš„ç³»ç»Ÿé…ç½®å¦‚ä½•ï¼Ÿè¿™ä¼šå½±å“æˆ‘ä»¬çš„æ¨¡å‹é€‰æ‹©å’Œä¼˜åŒ–ç­–ç•¥

ä½ è§‰å¾—ä»å“ªä¸ªæ–¹é¢å¼€å§‹å®æ–½æ¯”è¾ƒå¥½ï¼Ÿ
